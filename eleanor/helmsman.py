# ###
# ### Helmsman
# ### Controls the execution of orders generated by the navigator
# ### Tucker Ely, Douglas G. Moore, Cole Mathis
import multiprocessing
import os
import re
import shutil
import time
import queue

import numpy as np
import pandas as pd

from tqdm import tqdm

# from test_script import PROFILER

# ### local imports
from .hanger.db_comms import execute_query
from .hanger.db_comms import establish_database_connection, retrieve_records, get_column_names
from .hanger.eq36 import eq3, eq6
from .hanger.data0_tools import determine_species_set, data0_suffix  # , determine_ele_set
from .hanger.tool_room import mk_check_del_directory, mine_pickup_lines, grab_float
from .hanger.tool_room import grab_lines, grab_str, WorkingDirectory


def Helmsman(camp, ord_id=None, num_cores=os.cpu_count()):
    """
    Keeping with the naval terminaology:
        The Navigator charts where to go.

        Then the helmsman guides the ship there, using sailors to do the necessary work.

    Thus:
        Navigator decides the region of parameter space to be explored, by issuing orders that are
        written in the vs (variable space) table. Each order contains a collections of discrete vs
        points distributed about the parameter space. The dimension of the paramter space are not
        spatial, as with real ships on the ocean plying logitude and latitude, but are instead
        thermodyanmic (tempeature, pressure, total C, total Fe, etc.). Each point in this paramter
        space contains variables (dimensions) suffient to describes a closed thermodyanmic system.

        The goal of the helmsman is to solve for the equilibrium behaiovr of each of these points
        distributed about the variable space (vs).

        The Helmsman does this by spawning a small numebr of sailors (with number of sailors
        determined by system capabilites), assigning each a vs point that they will thermodynamic
        'solve' in sucession untill all points have been solved.

        Each vs point assigned to a siolr, contains enough infomration to define a closed
        thermodynamic system. The saiolr employs EQ3/6 to determine the equilibrium
        characteristic of the system defined by the vs point, thus generating an associated point
        in the equilibrium space (es) table.

    :param camp: loaded campaign
    :type camp: :class:`Campaign` instance

    :param ord_id: order number
    :type ord_id: int


    """

    with camp.working_directory():
        conn = establish_database_connection(camp)

        elements, aq_sp, solids, ss, gasses = determine_species_set(path='huffer/')

        # ### retrieve issued order 'ord_id'
        order_query = '''
            SELECT * FROM `vs`
            WHERE `code` = 0 AND `uuid` NOT IN (SELECT `uuid` FROM `es`)
        '''
        if ord_id is not None:
            order_query += f' AND `ord` = {ord_id}'
        rec = retrieve_records(conn, order_query)
        vs_col_names = get_column_names(conn, 'vs')
        es_col_names = get_column_names(conn, 'es')

        conn.close()

    if len(rec) == 0:
        msg = "The Helmsman found no unexecuted points"
        if ord_id is not None:
            msg += f" for order {ord_id}"
        print(msg)
        return

    date = time.strftime("%Y-%m-%d", time.gmtime())

    # ### build order-specific local working directory
    order_path = os.path.join(camp.campaign_dir, 'order_{}'.format(ord_id))
    mk_check_del_directory(order_path)
    os.chdir(order_path)

    start = time.time()  # for diagnostic times
    if ord_id is None:
        print(f"Processing all unfullfiled orders ({len(rec)} points)")
    else:
        print(f"Processing Order {ord_id} ({len(rec)} points)")

    with WorkingDirectory(order_path):

        # ### build vs/es queues
        queue_manager = multiprocessing.Manager()
        vs_queue = queue_manager.Queue()
        es_queue = queue_manager.Queue()

        keep_running_yoeman = multiprocessing.Value('b', True)
        yoeman_process = multiprocessing.Process(target=yoeman, args=(camp, keep_running_yoeman,
                                                 vs_queue, es_queue, len(rec)))
        yoeman_process.start()

        # # ##############   Multiprocessing  ##################
        with multiprocessing.Pool(processes=num_cores) as pool:

            _ = pool.starmap(sailor, zip([camp] * len(rec),
                                         [order_path] * len(rec),
                                         [vs_queue] * len(rec),
                                         [es_queue] * len(rec),
                                         [date] * len(rec),
                                         rec,
                                         [elements] * len(rec),
                                         [ss] * len(rec),
                                         [vs_col_names] * len(rec),
                                         [es_col_names] * len(rec)))

        # TODO
        # # ### check to see if queues are empty to kill
        # while not vs_queue.empty():
        #     time.sleep(1)
        # while not es_queue.empty():
        #     time.sleep(1)

        with keep_running_yoeman.get_lock():
            keep_running_yoeman.value = False

    yoeman_process.join()
    # vs_queue.join()
    # es_queue.join()

    print(f'\nOrder {ord_id} complete.')
    print(f'        total time: {round(time.time() - start, 4)}')
    print(f'        time/point: {round((time.time() - start) / len(rec), 4)}')
    print(f'   time/point/core: {round((num_cores * (time.time() - start)) / len(rec), 4)}\n')

def sailor(camp, order_path, vs_queue, es_queue, date, dat,
           elements, ss, vs_col_names, es_col_names, keep_every_n_files=1):
    """
    Each sailor manages the execution of all geochemically model steps associated
    with a single vs point in the Variable Space (VS).
    These steps included:

        (1) build 3i
        (2) run 3i
        (3) build 6i
        (4) run 6i
        (5) mine 6o

    :param camp: loaded campaign
    :type camp: :class:`Campaign` instance

    :param order_path: local directory path for the current order
    :type order_path: str

    :param vs_queue: a waiting list of lines that need to be written to vs table
    :type vs_queue: class 'multiprocessing.managers.AutoProxy[Queue]

    :param es_queue: a queue of pandas data frames that need to be written to es table
    :type es_queue: class 'multiprocessing.managers.AutoProxy[Queue]

    :param date: birthdate of order
    :type data: str

    :param dat: all vs specific data neede by teh sailor to complete mission.
    :type dat: list

    :param elements: list of loaded element, excepting O and H
    :type elements: list of strings

    :param ss: list of loaded solid solutions
    :type ss: list of strings

    :param vs_col_names: column headers in vs table. If a value in the eq3/6 run files is not in
                         the column names, then it is not captured.
    :type vs_col_names: list of strings

    :param es_col_names: column headers in es table. If a value in the eq3/6 run files is not in
                         the column names, then it is not captured.

    :type es_col_names: list of strings

    :param keep_every_n_files: keep every n (multiple) files. All run files get deleted after being
                               processed into the VS/ES sql database, as they are collectively very
                               large. A subset of files can be kept here for later manual
                               This arguemnt allows you to keep the raw data eq3/6 for a subset of
                               runs so that you can evaluate the output directly. the sql codes grab
                               alot of the eq3.6 run ionformaiton, but not all of it.
    :type keep_every_n_files: int

    """
    # PROFILER.enable()
    run_num = str(dat[3])
    file = '{}.3i'.format(run_num)

    if int(run_num) % keep_every_n_files == 1:
        delete_after_running = True
    else:
        delete_after_running = False
    # delete_after_running = True
    master_dict = {}
    for i, j in zip(vs_col_names, dat):
        master_dict[i] = j

    state_dict = {}
    for _ in camp.vs_state:
        state_dict[_] = master_dict[_]

    basis_dict = {}
    for _ in camp.vs_basis:
        basis_dict[_] = master_dict[_]

    rnt_dict = {}
    rnt_keys = list(camp.target_rnt.keys())

    for _ in range(len(rnt_keys)):
        name = rnt_keys[_]
        rnt_type = camp.target_rnt[name][0]
        morr = master_dict['{}_morr'.format(name)]
        rkb1 = master_dict['{}_rkb1'.format(name)]
        rnt_dict[name] = [rnt_type, morr, rkb1]

    # ### build and enter temp directory
    mk_check_del_directory(run_num)
    os.chdir(run_num)

    # ### select proper data0
    suffix = data0_suffix(state_dict['T_cel'], state_dict['P_bar'])

    # ### build and execute 3i
    camp.local_3i.write(file, state_dict, basis_dict, master_dict['cb'], output_details='n')
    data1_file = os.path.join(camp.data0_dir, "data1." + suffix)

    out, err = eq3(data1_file, file)  # TODO: update after dougs error handelign is ready.
    # above variables not currently used

    if not os.path.isfile(file[:-1] + 'p'):
        # ### check 3p not generated. Then rebuild 3i and rerun as
        # ### 'v = verbose' to generate diagnostics. This will of
        # ### course not help converge the file, but will provide the
        # ### infomration needed to map evidence of the failure for
        # ### future code to assess.
        reset_sailor(order_path, vs_queue, file, dat[0], 30,
                     delete_local=delete_after_running)
        return

    # ### process 3p file
    try:
        pickup = mine_pickup_lines('.', file[:-1] + 'p', 's')
    except Exception as e:
        # ### cannot mine pickup lines
        print('{}\n  {}\n'.format(file, e))
        reset_sailor(order_path, vs_queue, file, dat[0], 31,
                     delete_local=delete_after_running)
        return

    camp.local_6i.write(file[:-2] + '6i', rnt_dict, pickup, state_dict['T_cel'])
    out, err = eq6(data1_file, file[:-2] + '6i')
    if not os.path.isfile(file[:-2] + '6o'):
        reset_sailor(order_path, vs_queue, file, dat[0], 60,
                     delete_local=delete_after_running)

        return
    run_code, build_df = mine_6o(camp, date, elements, ss, file[:-2] + '6o', dat, es_col_names)
    if run_code == 100:
        # ### write to ES
        es_queue.put_nowait(build_df)

    reset_sailor(order_path, vs_queue, file, dat[0], run_code,
                 delete_local=delete_after_running)
    # mark_time = time.time()
    # print("Time through mark: ", mark_time - sailor_start_time)
    # PROFILER.disable()
    return None


def mine_6o(camp, date, elements, ss, file, dat, col_names):
    """
    open and mine the eq6 output file ('file'.6o) for all of the run information
    with associated columns in the ES table.

    :param camp: loaded campaign
    :type camp: :class:`Campaign` instance

    :param date: birthdate of order
    :type data: str

    :param elements: list of loaded element, excepting O and H
    :type elements: list of strings

    :param ss: list of loaded solid solutions
    :type ss: list of strings

    :param file: 'file'.6o file name
    :type file: str

    :param dat: all vs specific data neede by teh sailor to complete mission.
    :type dat: list

    :param col_names: ES table columns
    :type col_names: list of strings
    """

    build_dict = {k: [] for k in col_names}

    lines = grab_lines(file)  # 6o file

    run_code = 0  # default to un-run file '0'
    search_for_xi = False
    for _ in range(len(lines) - 1, 0, -1):
        # ## search from bottom of file
        if '---  The reaction path has terminated early ---' in lines[_]:
            # ## do not process 6o
            build_df = pd.DataFrame.from_dict(build_dict)
            return 70, build_df
        elif '---  The reaction path has terminated normally ---' in lines[_]:
            run_code = 100
            # ## Healthy file. Search for index of the last xi step
            search_for_xi = True
        elif search_for_xi and '                Log Xi=' in lines[_]:
            # ## The first appearnce of this, when searching from the bottom
            # ## is the final EQ step of interest for populating ES.
            last_xi_step_begins = _  # grab index for later.
            break

    if run_code == 0:
        # ## run code has not be altered, therefore unknown error
        build_df = pd.DataFrame.from_dict(build_dict)
        return 61, build_df

    # ## populate ES table
    for _ in range(len(lines)):
        if '   Affinity of the overall irreversible reaction=' in lines[_]:
            # ## the first instance of this line is xi = 0.0
            # ## (initial disequilibria with target mineral)
            build_dict["initial_aff"] = [grab_float(lines[_], -2)]
            break

    # ## search from beginning of last xi step (set in last_xi_step_begins)
    # ## grab xi_max. since intial index conatins log Xi.
    build_dict['xi_max'] = [grab_float(lines[last_xi_step_begins], -1)]

    for _ in range(last_xi_step_begins, len(lines)):
        if re.findall('^\n', lines[_]):
            pass

        elif ' Temperature=' in lines[_]:
            build_dict['T_cel'] = [grab_float(lines[_], -2)]

        elif ' Pressure=' in lines[_]:
            build_dict['P_bar'] = [grab_float(lines[_], -2)]

        elif ' --- Elemental Composition' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):
                if grab_str(lines[_ + x], 0) in elements:
                    # ### log molality
                    this_dat = [np.round(np.log10(grab_float(lines[_ + x], -1)), 6)]
                    build_dict['{}'.format(grab_str(lines[_ + x], 0))] = this_dat
                    x += 1
                else:
                    x += 1

        elif '                Log oxygen fugacity=' in lines[_]:
            build_dict['fO2'] = [grab_float(lines[_], -1)]

        elif '              Log activity of water=' in lines[_]:
            build_dict['aH2O'] = [grab_float(lines[_], -1)]

        elif '                 Ionic strength (I)=' in lines[_]:
            build_dict['ionic'] = [grab_float(lines[_], -2)]

        elif '                 Solutes (TDS) mass=' in lines[_]:
            build_dict['tds'] = [grab_float(lines[_], -2)]

        elif '              Aqueous solution mass=' in lines[_]:
            build_dict['soln_mass'] = [grab_float(lines[_], -2)]

        elif '--- Distribution of Aqueous Solute Species ---' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):
                if grab_str(lines[_ + x], 0) != 'O2(g)':
                    # ### -1 position is log activity, -3 is log molality
                    build_dict[grab_str(lines[_ + x], 0)] = [grab_float(
                        lines[_ + x], -1)]
                    x += 1
                else:
                    x += 1
            del x

        elif '--- Summary of Solid Phases (ES) ---' in lines[_]:
            # ### Solids data is stored in a temp_s_dict, to be
            # ### added to the build_df after the file is processed.
            # ### This way 'moles precipitated', if it exists, overwrites
            # ### the affinity data. See notes at the beginning of
            # ### this function for explanantion. This temporary
            # ### dictionary is only necessary because the solids
            # ### precipiation moles is reported ahead of the affinity
            # ### data in the 6o file.
            temp_s_dict = {}

            x = 4
            while not re.findall('^\n', lines[_ + x]):
                if 'None' not in lines[_ + x]:
                    # ## solids value grab_float()'s must reach from the
                    # ## end of the line (-1, -2, etc.)
                    # ## becuase some solid names contain spaces.
                    # ## Additionally, solid names are grabbed based
                    # ## in-line index as opposed to the grab_str()
                    # ## function, for the same reason.

                    # ### mols
                    temp_s_dict[lines[_ + x][:25].strip()] = [grab_float(lines[_ + x], -3)]
                    x += 1
                else:
                    x += 1
            del x

        elif '--- Saturation States of Pure Solids ---' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):
                if re.findall(r'\*{4}$', lines[_ + x]):
                    # ## '******'' fills in the value region for numbers
                    # ## lower than -999.9999. Replace with boundry condition

                    # ## affinity (kcal)
                    build_dict[lines[_ + x][:30].strip()] = [float(-999.9999)]
                    x += 1
                elif 'None' not in lines[_ + x]:
                    build_dict[lines[_ + x][:30].strip()] = [
                        float(lines[_ + x][44:55])]
                    x += 1
                else:
                    x += 1
            del x

        elif camp.SS and ' --- Saturation States of Solid Solutions ---' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):
                if re.findall(r'\*{4}$', lines[_ + x]):
                    # ## '******'' fills in the value region for numbers
                    # ## lower than -999.9999. Replace with boundry condition

                    # ## affinity (kcal)
                    build_dict[lines[_ + x][:30].strip()] = [float(-999.9999)]
                    x += 1
                elif 'None' not in lines[_ + x]:
                    # ## affinity (kcal)
                    build_dict[lines[_ + x][:30].strip()] = [
                        float(lines[_ + x][44:55])]
                    x += 1
                else:
                    x += 1
            del x

        elif '    --- Fugacities ---' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):

                if 'None' not in lines[_ + x]:  # log f
                    if re.findall(r'\*{4}', lines[_ + x]):
                        # ## '******'' fills in the value region for numbers
                        # ## lower than -999.9999. Replace with boundry condition

                        # ## affinity (kcal)
                        build_dict[lines[_ + x][:30].strip()] = [float(-999.9999)]
                        x += 1
                    else:
                        build_dict[grab_str(lines[_ + x], 0)] = [
                            float(lines[_ + x][28:41])]
                        x += 1
                else:
                    x += 1
            del x
            break

    for _ in temp_s_dict.keys():
        # ## write temp_s_dict[_] to build_df[_]. As temp_s_dict only
        # ## contains solids that actually precipitated, the
        # ## affinity vlaues in build_df[_] can simply be overwritten

        build_dict[_] = temp_s_dict[_]

    build_dict['uuid'] = [dat[0]]
    # build_dict['camp'] = [dat[1]]  # Removing the campaign name from the ES table
    build_dict['ord'] = [dat[2]]
    build_dict['file'] = [dat[3]]
    build_dict['run'] = [date]
    build_dict['mineral'] = [dat[5]]

    if not camp.SS:
        for _ in ss:
            build_dict[_] = [-999999]

    # ## reorganize columns to match es table
    build_df = pd.DataFrame.from_dict(build_dict)

    build_df = build_df[col_names]

    return run_code, build_df


def yoeman(camp, keep_running, write_vs_q, write_es_q, num_points):
    """
    Colecting each sailors dict output, and then writing it in
    bulk to sql

    :param camp: loaded campaign
    :type camp: :class:`Campaign` instance

    :param keep_running: should the yoeman continue to run (ie. is there shit
                         left to write to the VS/ES)
    :type keep_running: multiprocessing.value boolean

    :param write_vs_q: queue of lines waiting to be writen to vs table
    :type write_vs_q: queue.Queue

    :param write_es_q: queue of dataframes waiting to be writen to vs table
    :type write_es_q: queue.Queue

    :param num_points: number of vs points in the order
    :type num_points: int
    """
    conn = establish_database_connection(camp)
    WRITE_EVERY_N = 100
    vs_n_written = 0

    es_df_list = []
    vs_list = []

    if num_points <= WRITE_EVERY_N:
        write_all_at_once = True
    else:
        write_all_at_once = False
        progress = tqdm(total=num_points)

    while keep_running.value:
        # Get the current size
        current_q_size = write_vs_q.qsize()

        if current_q_size < WRITE_EVERY_N and not write_all_at_once:
            time.sleep(0.1)
            # print("Sleeping Yoeman")

        elif current_q_size == num_points and write_all_at_once:
            # Get everything written
            # Get VS Points
            while not write_vs_q.empty():
                vs_line = write_vs_q.get_nowait()
                vs_list.append(vs_line)
                write_vs_q.task_done()
            # Get ES points
            while not write_es_q.empty():
                es_df = write_es_q.get_nowait()
                es_df_list.append(es_df)
                write_es_q.task_done()
            # Write ES table
            total_es_df = pd.concat(es_df_list, ignore_index=True)
            total_es_df.to_sql('es', conn, if_exists='append', index=False)
            es_df_list = []
            # Write VS lines
            for vs_point in vs_list:
                execute_query(conn, vs_point)
                vs_n_written += 1

        elif not write_all_at_once and current_q_size > WRITE_EVERY_N:
            # Get VS batch
            while len(vs_list) < WRITE_EVERY_N:
                vs_line = write_vs_q.get_nowait()
                vs_list.append(vs_line)
                write_vs_q.task_done()
            # Write batch to VS table
            for vs_point in vs_list:
                execute_query(conn, vs_point)
                vs_n_written += 1
            vs_list = []
            # Get ES batch
            current_es_q_size = write_es_q.qsize()
            while len(es_df_list) < current_es_q_size:
                es_df = write_es_q.get_nowait()
                es_df_list.append(es_df)
                write_es_q.task_done()
            # Write batch to ES table
            total_es_df = pd.concat(es_df_list, ignore_index=True)
            total_es_df.to_sql('es', conn, if_exists='append', index=False)
            es_df_list = []
            progress.update(vs_n_written)

    # Clean up the last batch
    # Get remaining vs lines
    while not write_vs_q.empty():
        vs_line = write_vs_q.get_nowait()
        vs_list.append(vs_line)
        write_vs_q.task_done()
    # Write last batch to VS table
    for vs_point in vs_list:
        execute_query(conn, vs_point)
        vs_n_written += 1
    while not write_es_q.empty():
        es_df = write_es_q.get_nowait()
        es_df_list.append(es_df)
        write_es_q.task_done()
    # Write batch to ES table
    if es_df_list != []:
        total_es_df = pd.concat(es_df_list, ignore_index=True)
        total_es_df.to_sql('es', conn, if_exists='append', index=False)
    progress.update(vs_n_written)
    return None

def six_o_data_to_sql(conn, table, df):
    """
    Commit the 6o dataframe to sql

    :param conn: sql database connnection
    :type conn: :class: sqlite3.Connection

    :param table: name of sql table
    :type table: str

    :param df: datafram contain the 6o information to written to the es table
    :type df: 'pandas.core.frame.DataFrame'

    """
    df.to_sql(table, conn, if_exists='append', index=False)


def reset_sailor(order_path, vs_queue, file, uuid, code, delete_local=False):
    """
    The sailor is finished, for better or worse. Close up shop.

    (1) Report to vs table 'camp_name' via server connection 'conn' the
        exit 'code' for 'file' with unique vs_table id 'uuid'.
    (2) Step back into order folder 'order_path' for next vs point.

    :param order_path: path to order folder
    :type order_path: str

    :param vs_queue: a waiting list of lines that need to be written to vs table
    :type vs_queue: class 'multiprocessing.managers.AutoProxy[Queue]

    :param file: original 3i file name for this vs point 'id_number.3i'
    :type file: str

    :param uuid: 'Universally Unique IDentifier'
    :type uuid: str

    :param code: custom exit codes describing eq3/6 errors
    :type code: int

    :param delete_local: do you want to keep the local folder full of rthe eq3/6 files?
    :type delete_local: boolean

    """
    # This only takes like 1e-5 seconds even with the put
    sql = """UPDATE vs SET code = {} WHERE uuid = '{}';""".format(code, uuid)
    vs_queue.put_nowait(sql)
    os.chdir(order_path)
    if delete_local:
        shutil.rmtree(file[:-3])