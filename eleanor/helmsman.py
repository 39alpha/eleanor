# ###
# ### Helmsman
# ### Controls the execution of orders generated by the navigator
# ### Tucker Ely, Douglas G. Moore, Cole Mathis

import multiprocessing
import os, sys
import re
import shutil
import time
import queue

import numpy as np
import pandas as pd

from tqdm import tqdm

# ### custom packages
from .hanger.db_comms import execute_query
from .hanger.db_comms import establish_database_connection, retrieve_records, get_column_names
from .hanger.eq36 import eq3, eq6
from .hanger.data0_tools import determine_ele_set, determine_species_set, data0_suffix
from .hanger.tool_room import mk_check_del_directory, mine_pickup_lines, grab_float
from .hanger.tool_room import grab_lines, grab_str, WorkingDirectory


def Helmsman(camp, ord_id=None):
    """
    Keeping with the naval terminaology:
        The Navigator charts where to go.

        Then the helmsman guides the ship there, using sailors to do the necessary work.

    Thus:
        Navigator decides the region of parameter space to be explored, by issuing orders that are
        written in the vs (variable space) table. Each order contains a collections of discrete vs
        points distributed about the parameter space. The dimension of the paramter space are not
        spatial, as with real ships on the ocean plying logitude and latitude, but are instead
        thermodyanmic (tempeature, pressure, total C, total Fe, etc.). Each point in this paramter
        space contains variables (dimensions) suffient to describes a closed thermodyanmic system.

        The goal of the helmsman is to solve for the equilibrium behaiovr of each of these points
        distributed about the variable space (vs).

        The Helmsman does this by spawning a small numebr of sailors (with number of sailors
        determined by system capabilites), assigning each a vs point that they will thermodynamic
        'solve' in sucession untill all points have been solved.

        Each vs point assigned to a siolr, contains enough infomration to define a closed
        thermodynamic system. The saiolr employs EQ3/6 to determine the equilibrium
        characteristic of the system defined by the vs point, thus generating an associated point
        in the equilibrium space (es) table.

    """

    with camp.working_directory():
        conn = establish_database_connection(camp)
        # elements = determine_ele_set(path='huffer/')

        elements, aq_sp, solids, ss, gasses = determine_species_set(path='huffer/')

        # ### retrieve issued order 'ord_id'
        order_query = '''
            SELECT * FROM `vs`
            WHERE `code` = 0 AND `uuid` NOT IN (SELECT `uuid` FROM `es`)
        '''
        if ord_id is not None:
            order_query += f' AND `ord` = {ord_id}'
        rec = retrieve_records(conn, order_query)

        vs_col_names = get_column_names(conn, 'vs')
        es_col_names = get_column_names(conn, 'es')

        conn.close()

    if len(rec) == 0:
        msg = "The Helmsman found no unexecuted points"
        if ord_id is not None:
            msg += f" for order {ord_id}"
        print(msg)
        return

    # ### date time stamp for run date
    date = time.strftime("%Y-%m-%d", time.gmtime())

    # ### build order specific local working directory
    order_path = os.path.join(camp.campaign_dir, 'order_{}'.format(ord_id))

    # ### create local directory for samaple run
    mk_check_del_directory(order_path)
    os.chdir(order_path)

    start = time.time()
    if ord_id is None:
        print(f"Processing all unfullfiled orders ({len(rec)} points)")
    else:
        print(f"Processing Order {ord_id} ({len(rec)} points)")
    cores = 4  # TODO: This doesn't make no damn sense, detect or pass as an argument

    with WorkingDirectory(order_path):
        # ### build vs/es queues
        queue_manager = multiprocessing.Manager()

        vs_queue = queue_manager.Queue()
        es_queue = queue_manager.Queue()

        keep_running_yoeman = multiprocessing.Value('b', True)
        yoeman_process = multiprocessing.Process(target=yoeman, args=(camp, keep_running_yoeman,
                                                 vs_queue, es_queue, len(rec)))
        yoeman_process.start()
        # # ##############   Multiprocessing  ##################

        with multiprocessing.Pool(processes=cores) as pool:
            _ = pool.starmap(sailor, zip([camp] * len(rec),
                                         [order_path] * len(rec),
                                         [vs_queue] * len(rec),
                                         [es_queue] * len(rec),
                                         [date] * len(rec),
                                         rec,
                                         [elements] * len(rec),
                                         [ss] * len(rec),
                                         [vs_col_names] * len(rec),
                                         [es_col_names] * len(rec)))
        # # ### check to see if queues are empty to kill

        # ### Testing with serial computation for easier error reporting
        # for r in rec[:100]:
        #     sailor(camp, order_path, vs_queue, es_queue, date, r, elements, vs_col_names, es_col_names)

        while not vs_queue.empty():
            time.sleep(1)
        while not es_queue.empty():
            time.sleep(1)

        with keep_running_yoeman.get_lock():
            keep_running_yoeman.value = False

    print('\nOrder {} complete. Debugging Dont believe these times'.format(ord_id))
    print('        total time: {}'.format(round(time.time() - start, 4)))
    print('     time/point: {}'.format(round((time.time() - start) / len(rec), 4)))
    print('time/point/core: {}\n'.format(round((cores * (time.time() - start)) / len(rec), 4)))

    yoeman_process.terminate()


def sailor(camp, order_path, vs_queue, es_queue, date, dat, elements, ss, vs_col_names, es_col_names):
    """
    Run system 'run', a point (vs) in variable space (VS) retireved from vs table
    (1) build 3i
    (2) run 3i
    (3) build 6i
    (4) run 6i
    (5) mine 6o
    """

    # ord_id = str(dat[2])
    run_num = str(dat[3])
    file = '{}.3i'.format(run_num)

    # ### determine if the run folder will be kept for later evaluation
    # ### this functionality was installed in order to limit the quantity
    # ### of data generated during large runs. If a specifc output file is desired
    # ### it can simply be rerun from the data in the vs table and the campaign sheet.
    keep_every_n_files = 1000
    if int(run_num) in [int(idx) for idx in np.arange(1, 10000000, keep_every_n_files)]:
        delete_after_running = False
    else:
        delete_after_running = True

    master_dict = {}
    for i, j in zip(vs_col_names, dat):
        master_dict[i] = j

    state_dict = {}
    for _ in camp.vs_state:
        state_dict[_] = master_dict[_]

    basis_dict = {}
    for _ in camp.vs_basis:
        basis_dict[_] = master_dict[_]

    rnt_dict = {}
    rnt_keys = list(camp.target_rnt.keys())

    for _ in range(len(rnt_keys)):
        name = rnt_keys[_]

        # ### TODO: add rock reactant cpaability, partial below
        # if '.json' in name:
        #     name = name[:-5]
        #     morr = master_dict['{}_morr'.format(name)]
        #     rkb1 = master_dict['{}_rkb1'.format(name)]

        #     rock_ele_dict = {}
        #     ele_in_rock = rock_dat.grab_rock_ele()
        #     for _ in ele_in_rock:
        #         rock_ele_dict[_] = master_dict['{}_morr'.format(_)]
        #     # ### pass elements in 2 spot
        #     rnt_dict['whole_rock'] = [morr, rkb1, rock_ele_dict]

        # else:

        rnt_type = camp.target_rnt[name][0]
        morr = master_dict['{}_morr'.format(name)]
        rkb1 = master_dict['{}_rkb1'.format(name)]
        rnt_dict[name] = [rnt_type, morr, rkb1]

    # ### build and enter temp directory
    mk_check_del_directory(run_num)
    os.chdir(run_num)

    # ### select proper data0
    suffix = data0_suffix(state_dict['T_cel'], state_dict['P_bar'])

    # ### build and execute 3i
    camp.local_3i.write(file, state_dict, basis_dict, output_details='n')
    data1_file = os.path.join(camp.data0_dir, "data1." + suffix)
    out, err = eq3(data1_file, file)  # TODO: Look at yourself, what the fuck.

    # ### check 3p and 3o to determine system status
    if not os.path.isfile(file[:-1] + 'p'):
        # ### check 3p not generated. Then rebuild 3i and rerun as
        # ### 'v = verbose' to generate diagnostics. This will of
        # ### course not help converge the file, but will provide the
        # ### infomration needed to map evidence of the failure for
        # ### future code to assess.
        reset_sailor(order_path, vs_queue, file, dat[0], 30,
                     delete_local=delete_after_running)
        return

    # ### process 3p file
    try:
        pickup = mine_pickup_lines('.', file[:-1] + 'p', 's')
    except Exception as e:
        # ### cannot mine pickup lines
        print('{}\n  {}\n'.format(file, e))
        reset_sailor(order_path, vs_queue, file, dat[0], 31,
                     delete_local=delete_after_running)
        return

    camp.local_6i.write(file[:-2] + '6i', rnt_dict, pickup, state_dict['T_cel'])
    out, err = eq6(data1_file, file[:-2] + '6i')

    # ### check that 6o was generated
    if not os.path.isfile(file[:-2] + '6o'):
        reset_sailor(order_path, vs_queue, file, dat[0], 60,
                     delete_local=delete_after_running)

        return

    # ### mine 6o and record in es if complete
    run_code, build_df = mine_6o(camp, date, elements, ss, file[:-2] + '6o', dat, es_col_names)

    # ### load into es_table
    if run_code == 100:
        # ### push bould_df onto eq_q
        es_queue.put(build_df)
        # ### old #### six_o_data_to_sql(conn, 'es', build_df)

    reset_sailor(order_path, vs_queue, file, dat[0], run_code,
                 delete_local=delete_after_running)

def mine_6o(camp, date, elements, ss, file, dat, col_names):
    """
    conn = open postgresql connection to database with camp.__ tables
    date = run date for file
    file = local 6o file or file path
    dat = data from orders issueed for file
    col_names = ES table columns, in correct order (arrangement).
    """
    # ## initiate values dataframe with the full column name list.
    # ## note: each solid only has one column. Affinity is written,
    # ## and then moles precipiated is written over it, if it exists.

    # ## Therefore, negative values always equal affinities, but
    # ## positive values could either be positive affinities, or
    # ## moles precipitated. However, in any given system they can
    # ## never be both. If precipitation is allowed, then posative
    # ## values equal moles, if precipiatation for a given pahse is
    # ## inhibited, then posative values equals affinties
    # build_df = pd.DataFrame(columns=col_names)
    build_dict = {k: [] for k in col_names}
    # ## 6o file as a list of strings
    lines = grab_lines(file)

    # ## handel braod exit conditions
    run_code = 0  # default to un-run file '0'
    search_for_xi = False
    for _ in range(len(lines) - 1, 0, -1):
        # ## search from bottom of file
        if '---  The reaction path has terminated early ---' in lines[_]:
            # ## do not process 6o
            build_df = pd.DataFrame.from_dict(build_dict)
            return 70, build_df
        elif '---  The reaction path has terminated normally ---' in lines[_]:
            run_code = 100
            # ## Healthy file. Search for index of the last xi step
            search_for_xi = True
        elif search_for_xi and '                Log Xi=' in lines[_]:
            # ## The first appearnce of this, when searching from the bottom
            # ## is the final EQ step of interest for populating ES.
            last_xi_step_begins = _  # grab index for later.
            break

    if run_code == 0:
        # ## run code has not be altered, therefore unknown error
        # ## has occured
        build_df = pd.DataFrame.from_dict(build_dict)
        return 61, build_df

    # ## grab data and populate ES table
    for _ in range(len(lines)):
        if '   Affinity of the overall irreversible reaction=' in lines[_]:
            # ## the first instance of this line is xi = 0.0
            # ## (initial disequilibria with target mineral)
            build_dict["initial_aff"] = [grab_float(lines[_], -2)]
            break

    # ## search from beginning of last xi step (set in last_xi_step_begins)
    # ## grab xi_max. since intial index conatins log Xi.
    # build_df['xi_max'] = [grab_float(lines[last_xi_step_begins], -1)]
    build_dict['xi_max'] = [grab_float(lines[last_xi_step_begins], -1)]

    for _ in range(last_xi_step_begins, len(lines)):

        if re.findall('^\n', lines[_]):
            # ## efficincy test. I thin this if statemement will catch
            # ## all of the empty lines, without having to cycle through
            # ## all of the below elif statements to discover that none
            # ## of them work.
            pass

        elif ' Temperature=' in lines[_]:
            build_dict['T_cel'] = [grab_float(lines[_], -2)]

        elif ' Pressure=' in lines[_]:
            build_dict['P_bar'] = [grab_float(lines[_], -2)]

        elif ' --- Elemental Composition' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):
                if grab_str(lines[_ + x], 0) in elements:
                    # ## log molality data
                    this_dat = [np.round(np.log10(grab_float(lines[_ + x], -1)), 6)]
                    build_dict['{}'.format(grab_str(lines[_ + x], 0))] = this_dat
                    x += 1
                else:
                    x += 1

        elif '                Log oxygen fugacity=' in lines[_]:
            build_dict['fO2'] = [grab_float(lines[_], -1)]

        elif '              Log activity of water=' in lines[_]:
            build_dict['aH2O'] = [grab_float(lines[_], -1)]

        elif '                 Ionic strength (I)=' in lines[_]:
            build_dict['ionic'] = [grab_float(lines[_], -2)]

        elif '                 Solutes (TDS) mass=' in lines[_]:
            build_dict['tds'] = [grab_float(lines[_], -2)]

        elif '              Aqueous solution mass=' in lines[_]:
            build_dict['soln_mass'] = [grab_float(lines[_], -2)]

        elif '--- Distribution of Aqueous Solute Species ---' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):
                if grab_str(lines[_ + x], 0) != 'O2(g)':
                    build_dict[grab_str(lines[_ + x], 0)] = [grab_float(
                        lines[_ + x], -1)]
                    x += 1
                else:
                    x += 1
            del x

        elif '--- Summary of Solid Phases (ES) ---' in lines[_]:
            # ###    Solids precip data is stored in a temp_s_dict, to be
            # ### added to the build_df after the file is processed.
            # ### This way precipitation moles overwrites the affinity
            # ### data, only if it exists. See notes at the beginning of
            # ### this function for explanantion. This temporary
            # ### dictionary is only necessary because the solids
            # ### precipiation moles is reported ahead of the affinity
            # ### data in the 6o file.
            temp_s_dict = {}

            x = 4     # lines offset from beginning of sp data
            while not re.findall('^\n', lines[_ + x]):
                if 'None' not in lines[_ + x]:
                    # ## solids value grab_float()'s must reach from the
                    # ## end of the line (-1, -2, etc.)
                    # ## becuase some solid names contain spaces.
                    # ## Additionally, solid names are grabbed based
                    # ## in-line index as opposed to the grab_str()
                    # ## function, for the same reason.

                    # ### mols
                    temp_s_dict[lines[_ + x][:25].strip()] = [grab_float(lines[_ + x], -3)]
                    x += 1
                else:
                    x += 1
            del x

        elif '--- Saturation States of Pure Solids ---' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):
                if re.findall(r'\*{4}$', lines[_ + x]):
                    # ## ****** fills in the value region for numbers
                    # ## lower than -999.9999. So I am replacing them
                    # ## here with the boundry condition.

                    # ## affinity (kcal)
                    # build_df[lines[_ + x][:30].strip()] = [float(-999.9999)]
                    build_dict[lines[_ + x][:30].strip()] = [float(-999.9999)]
                    x += 1
                elif 'None' not in lines[_ + x]:
                    # ##    affinity (kcal)
                    # build_df[lines[_ + x][:30].strip()] = [
                    #    float(lines[_ + x][44:55])]
                    build_dict[lines[_ + x][:30].strip()] = [
                        float(lines[_ + x][44:55])]
                    x += 1
                else:
                    x += 1
            del x

        elif camp.SS and ' --- Saturation States of Solid Solutions ---' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):
                if re.findall(r'\*{4}$', lines[_ + x]):
                    # ## ****** fills in the value region for numbers
                    # ## lower than -999.9999, So I am replacing them
                    # ## here with the boundry condition.

                    # ## affinity (kcal)
                    # build_df[lines[_ + x][:30].strip()] = [float(-999.9999)]
                    build_dict[lines[_ + x][:30].strip()] = [float(-999.9999)]
                    x += 1
                elif 'None' not in lines[_ + x]:
                    # ## affinity (kcal)
                    # build_df[lines[_ + x][:30].strip()] = [
                    #     float(lines[_ + x][44:55])]
                    build_dict[lines[_ + x][:30].strip()] = [
                        float(lines[_ + x][44:55])]
                    x += 1
                else:
                    x += 1
            del x

        elif '    --- Fugacities ---' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):

                if 'None' not in lines[_ + x]:  # log f
                    if re.findall(r'\*{4}', lines[_ + x]):
                        # ## ****** fills in the value region for numbers
                        # ## lower than -999.9999 and for gasses that have
                        # ## been user suppressed. So I am replacing them
                        # ## here with the boundry condition.

                        # ## affinity (kcal)
                        # build_df[lines[_ + x][:30].strip()] = [float(-999.9999)]
                        build_dict[lines[_ + x][:30].strip()] = [float(-999.9999)]
                        x += 1
                    else:
                        # build_df[grab_str(lines[_ + x], 0)] = [
                        #    float(lines[_ + x][28:41])]
                        build_dict[grab_str(lines[_ + x], 0)] = [
                            float(lines[_ + x][28:41])]
                        x += 1
                else:
                    x += 1
            del x
            break

    # ## combine temp_s_dict containing solid moles precip, and the
    # ## build_df, which continas the affinity data.
    for _ in temp_s_dict.keys():
        # ## write temp_s_dict[_] to build_df[_]. As temp_s_dict only
        # ## contains solids that actually precipitated, the
        # ## affinity vlaues in build_df[_] can simply be overwritten
        # build_df[_] = temp_s_dict[_]
        build_dict[_] = temp_s_dict[_]

    # ## set remaining es table variables
    # build_df['uuid'] = [dat[0]]
    # build_df['camp'] = [dat[1]]
    # build_df['ord'] = [dat[2]]
    # build_df['file'] = [dat[3]]
    # build_df['run'] = [date]
    # build_df['mineral'] = [dat[5]]

    build_dict['uuid'] = [dat[0]]
    build_dict['camp'] = [dat[1]]
    build_dict['ord'] = [dat[2]]
    build_dict['file'] = [dat[3]]
    build_dict['run'] = [date]
    build_dict['mineral'] = [dat[5]]

    # ### if solid solutions are turned off, then NaN is suppplied in their
    # ### place this is done, instead of removing them altogether, so that
    # ### difffernet orders in teh same campaign can turn them on and off,
    # ### without effecting the es table columns
    if not camp.SS:
        for _ in ss:
            build_dict[_] = [-999999]

    # ## reorganize columns to match es table
    build_df = pd.DataFrame.from_dict(build_dict)

    build_df = build_df[col_names]

    return run_code, build_df


def yoeman(camp, keep_running, write_vs_q, write_es_q, num_points):
    """
    Colecting each sailors dict output, and then writing it in
    bulk to sql
    """
    conn = establish_database_connection(camp)

    vs_n_written = 0

    progress = tqdm(total=num_points)
    while keep_running:

        # ### check that es has something to write
        try:
            es_df = write_es_q.get_nowait()
        except queue.Empty:
            pass
        else:
            # ### write es data to sql
            es_df.to_sql('es', conn, if_exists='append', index=False)
            progress.update()

        # ### check that vs has something to write
        try:
            vs_sql = write_vs_q.get_nowait()
        except queue.Empty:
            pass
        else:
            # ### write vs data to sql
            execute_query(conn, vs_sql)
            vs_n_written += 1



def six_o_data_to_sql(conn, table, df):
    df.to_sql(table, conn, if_exists='append', index=False)


def reset_sailor(order_path, vs_queue, file, uuid, code, delete_local=False):
    """
    The sailor is finished, for better or worse, with run number 'file'
    with exid code 'code'

    (1) Report to vs table 'camp_name' via server connection 'conn' the
        exit 'code' for 'file' with unique vs_table id 'uuid'.
    (2) Step back into order folder 'order_path' for next vs point.
    """
    sql = """UPDATE vs SET code = {} WHERE uuid = '{}';""".format(code, uuid)

    # ### push sql onto yoeman for submission
    # execute_query(conn, sql)

    vs_queue.put(sql)

    os.chdir(order_path)

    if delete_local:
        shutil.rmtree(file[:-3])
