# ###
# ### Helmsman
# ### Controls the execution of orders generated by the navigator
# ### Tucker Ely, Douglas G. Moore, Cole Mathis

import sys
import os
import time
import multiprocessing
import re
import numpy as np
import pandas as pd

# ### custom packages
# ### rebuild without *
from .hanger.eq36 import eq3, eq6
from .hanger.db_comms import establish_database_connection, retrieve_records
from .hanger.data0_tools import determine_ele_set, data0_suffix
from .hanger.tool_room import mk_check_del_directory, mine_pickup_lines, reset_sailor, grab_float
from .hanger.tool_room import grab_lines, grab_str, WorkingDirectory
# TODO: MOVE RESET_SAILOR INTO THE SAME FILE AS SAILOR

import eleanor.campaign as campaign

def main(camp, ord_id=None):
    """TODO: What does the helmsmen do??"""

    with camp.working_directory():
        conn = establish_database_connection(camp)
        elements = determine_ele_set(path='huffer/')

        # ### retrieve issued order 'ord_id'
        rec = retrieve_records(conn, "select * from vs where ord = {}".format(ord_id))

        try:
            # ### retrieve es col names for this specific campaign
            cursor = conn.cursor()
            cursor.execute("Select * FROM es LIMIT 0")
            col_names = [_[0] for _ in cursor.description]
            cursor.close()
        except Exception as error:  # TODO: better error handling
            print("Error while fetching data from SQL", error)
            cursor.close()
            print('  SQL couldnt understand whatever bullshit')
            print('  you were trying to tell it.\n')
            sys.exit()

        conn.close()

    # ### date time stamp for run date
    date = time.strftime("%Y-%m-%d", time.gmtime())

    # ### build order specific local working directory
    order_path = os.path.join(camp.campaign_dir, 'order_{}'.format(ord_id))

    mk_check_del_directory(order_path)
    os.chdir(order_path)

    start = time.time()
    print('Processing Order {}'.format(ord_id))
    cores = 1  # TODO: This doesn't make no damn sense, detect or pass as an argument

    with WorkingDirectory(order_path):
        ###############   Multiprocessing  ##################
        with multiprocessing.Pool(processes=cores) as pool:
            _ = pool.starmap(sailor, zip([camp] * len(rec),
                                         [order_path] * len(rec),
                                         [date] * len(rec),
                                         rec,
                                         [elements] * len(rec),
                                         [col_names] * len(rec)))

        # Testing with serial computation for easier error reporting
        # for r in rec[:100]:
        #     sailor(camp, order_path, date, r, elements, col_names)

    print('\nOrder {} complete. Debugging Dont believe these times'.format(ord_id))
    print('        total time: {}'.format(round(time.time() - start, 4)))
    print('     time/point: {}'.format(round((time.time() - start) / len(rec), 4)))
    print('time/point/core: {}\n'.format(round((cores * (time.time() - start)) / len(rec), 4)))


def sailor(camp, order_path, date, dat, elements, col_names):
    """
    Run system 'run', a point (vs) in variable space (VS) retireved from vs table
    (1) build 3i
    (2) run 3i
    (3) build 6i
    (4) run 6i
    (5) mine 6o
    """
    print('a')
    start = time.time()

    conn = establish_database_connection(camp)

    # ord_id = str(dat[2])
    run_num = str(dat[3])
    file = '{}.3i'.format(run_num)

    # ### determine if the run folder will be kept for later evaluation
    # ### this functionality was installed in order to limit the quantity
    # ### of data generated during large runs. If a specifc output file is desired
    # ### it can simply be rerun fromt he data in the vs table and the campaign sheet.
    keep_every_n_files = 50
    if int(run_num) in [int(idx) for idx in np.arange(1, 1000000, keep_every_n_files)]:
        delete_after_running = False
    else:
        delete_after_running = True

    # ### weave rnt/basis/state names with thier values from the current
    # ### order entry. This feels inefficient. There must be room for
    # ### optimization here
    rnt_n = len(camp.target_rnt.keys())
    state_n = len(camp.vs_state.keys())
    basis_n = len(camp.vs_basis.keys())

    rnt_dict = {}
    rnt_keys = list(camp.target_rnt.keys())

    for _ in range(len(rnt_keys)):
        name = rnt_keys[_]
        rnt_type = camp.target_rnt[name][0]
        morr = dat[6 + _]
        rk1b = dat[6 + rnt_n + _]
        # ### this works becuase dicts in py3.6 on, maintain insertion order
        rnt_dict[name] = [rnt_type, morr, rk1b]

    state_dict = {}
    state_start = 6 + 2 * rnt_n
    state_end = 6 + 2 * rnt_n + state_n
    for i, j in zip(camp.vs_state.keys(), dat[state_start: state_end]):
        state_dict[i] = j

    basis_dict = {}
    basis_start = 6 + 2 * rnt_n + state_n
    basis_end = 6 + 2 * rnt_n + state_n + basis_n
    for i, j in zip(camp.vs_basis.keys(), dat[basis_start: basis_end]):
        basis_dict[i] = j

    # ### build and enter temp directory
    mk_check_del_directory(run_num)
    os.chdir(run_num)

    print('b')
    # ### select proper data0
    suffix = data0_suffix(state_dict['T_cel'], state_dict['P_bar'])

    # ### build and execute 3i
    camp.local_3i.write(file, state_dict, basis_dict, output_details='n')
    data1_file = os.path.join(camp.data0dir, "data1." + suffix)
    out, err = eq3(data1_file, file)  # TODO: Look at yourself, what the fuck.

    # ### check 3p and 3o to determine system status
    if not os.path.isfile(file[:-1] + 'p'):
        # ### check 3p not generated. Then rebuild 3i and rerun as
        # ### 'v = verbose' to generate diagnostics. This will of
        # ### course not help converge the file, but will provide the
        # ### infomration needed to map evidence of the failure for
        # ### future code to assess.
        reset_sailor(order_path, start, conn,
                     camp.name, file, dat[0],
                     30,
                     delete_local=delete_after_running)
        return
    print('c')
    # ### process 3p file
    try:
        pickup = mine_pickup_lines('.', file[:-1] + 'p', 's')
    except Exception as e:
        # ### cannot mine pickup lines
        print('{}\n  {}\n'.format(file, e))
        reset_sailor(order_path, start, conn,
                     camp.name, file, dat[0],
                     31,
                     delete_local=delete_after_running)
        return

    camp.local_6i.write(file[:-2] + '6i', rnt_dict, pickup, state_dict['T_cel'])
    out, err = eq6(data1_file, file[:-2] + '6i')

    # ### check that 6o was generated
    if not os.path.isfile(file[:-2] + '6o'):
        reset_sailor(order_path, start, conn,
                     camp.name, file, dat[0],
                     60,
                     delete_local=delete_after_running)
        return

    # ### mine 6o and record in es if complete
    run_code, build_df = mine_6o(
        conn, date, order_path, elements, file[:-2] + '6o', dat, col_names)

    # ### load into es_table
    if run_code == 100:
        six_o_data_to_sql(conn, '{}_es'.format(camp.name), build_df)

    reset_sailor(order_path, start, conn,
                 camp.name, file, dat[0],
                 run_code,
                 delete_local=delete_after_running)

def mine_6o(conn, date, order_path, elements, file, dat, col_names):
    """
    conn = open postgresql connection to database with camp.__ tables
    date = run date for file
    file = local 6o file or file path
    dat = data from orders issueed for file
    col_names = ES table columns, in correct order (arrangement).
    """
    # ## initiate values dataframe with the full column name list.
    # ## note: each solid only has one column. Affinity is written,
    # ## and then moles precipiated is written over it, if it exists.

    # ## Therefore, negative values always equal affinities, but
    # ## positive values could either be positive affinities, or
    # ## moles precipitated. However, in any given system they can
    # ## never be both. If precipitation is allowed, then posative
    # ## values equal moles, if precipiatation for a given pahse is
    # ## inhibited, then posative values equals affinties

    build_df = pd.DataFrame(columns=col_names)

    # ## 6o file as a list of strings
    lines = grab_lines(file)

    # ## handel braod exit conditions
    run_code = 0  # default to un-run file '0'
    search_for_xi = False
    for _ in range(len(lines) - 1, 0, -1):
        # ## search from bottom of file
        if '---  The reaction path has terminated early ---' in lines[_]:
            # ## do not process 6o
            return 70, build_df
        elif '---  The reaction path has terminated normally ---' in lines[_]:
            run_code = 100
            # ## Healthy file. Search for index of the last xi step
            search_for_xi = True
        elif search_for_xi and '                Log Xi=' in lines[_]:
            # ## The first appearnce of this, when searching from the bottom
            # ## is the final EQ step of interest for populating ES.
            last_xi_step_begins = _  # grab index for later.
            break

    if run_code == 0:
        # ## run code has not be altered, therefore unknown error
        # ## has occured
        return 61, build_df

    # ## grab data and populate ES table
    for _ in range(len(lines)):
        if '   Affinity of the overall irreversible reaction=' in lines[_]:
            # ## the first instance of this line is xi = 0.0
            # ## (initial disequilibria with target mineral)

            # ##  kcal/mol
            build_df['initial_aff'] = [grab_float(lines[_], -2)]
            break

    # ## search from beginning of last xi step (set in last_xi_step_begins)
    # ## grab xi_max. since intial index conatins log Xi.
    build_df['xi_max'] = [grab_float(lines[last_xi_step_begins], -1)]

    for _ in range(last_xi_step_begins, len(lines)):

        if re.findall('^\n', lines[_]):
            # ## efficincy test. I thin this if statemement will catch
            # ## all of the empty lines, without having to cycle through
            # ## all of the below elif statements to discover that none
            # ## of them work.
            pass

        elif ' Temperature=' in lines[_]:
            build_df['T_cel'] = [grab_float(lines[_], -2)]

        elif ' Pressure=' in lines[_]:
            build_df['P_bar'] = [grab_float(lines[_], -2)]

        elif ' --- Elemental Composition' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):
                if grab_str(lines[_ + x], 0) in elements:
                    # ## log molality data
                    this_dat = [np.round(np.log10(grab_float(lines[_ + x], -1)), 6)]
                    build_df['{}'.format(grab_str(lines[_ + x], 0))] = this_dat
                    x += 1
                else:
                    x += 1

        elif '                Log oxygen fugacity=' in lines[_]:
            # ##    log fO2
            build_df['fO2'] = [grab_float(lines[_], -1)]

        elif '              Log activity of water=' in lines[_]:
            # ##    log aH2O
            build_df['aH2O'] = [grab_float(lines[_], -1)]

        elif '                 Ionic strength (I)=' in lines[_]:
            # ##    molal
            build_df['ionic'] = [grab_float(lines[_], -2)]

        elif '                 Solutes (TDS) mass=' in lines[_]:
            # ##    grams
            build_df['tds'] = [grab_float(lines[_], -2)]

        elif '              Aqueous solution mass=' in lines[_]:
            # ## grams
            build_df['soln_mass'] = [grab_float(lines[_], -2)]

        elif '--- Distribution of Aqueous Solute Species ---' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):
                if grab_str(lines[_ + x], 0) != 'O2(g)':
                    # ##    loga
                    build_df[grab_str(lines[_ + x], 0)] = [grab_float(
                        lines[_ + x], -1)]
                    x += 1
                else:
                    x += 1
            del x

        elif '--- Summary of Solid Phases (ES) ---' in lines[_]:
            # ###    Solids precip data is stored in a temp_s_dict, to be
            # ### added to the build_df after the file is processed.
            # ### This way precipitation moles overwrites the affinity
            # ### data, only if it exists. See notes at the beginning of
            # ### this function for explanantion. This temporary
            # ### dictionary is only necessary because the solids
            # ### precipiation moles is reported ahead of the affinity
            # ### data in the 6o file.
            temp_s_dict = {}

            x = 4     # lines offset from beginning of sp data
            while not re.findall('^\n', lines[_ + x]):
                if 'None' not in lines[_ + x]:
                    # ## solids value grab_float()'s must reach from the
                    # ## end of the line (-1, -2, etc.)
                    # ## becuase some solid names contain spaces.
                    # ## Additionally, solid names are grabbed based
                    # ## in-line index as opposed to the grab_str()
                    # ## function, for the same reason.

                    # ### mols
                    temp_s_dict[lines[_ + x][:25].strip()] = [grab_float(lines[_ + x], -3)]
                    x += 1
                else:
                    x += 1
            del x

        elif '--- Saturation States of Pure Solids ---' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):
                if re.findall(r'\*{4}$', lines[_ + x]):
                    # ## ****** fills in the value region for numbers
                    # ## lower than -999.9999. So I am replacing them
                    # ## here with the boundry condition.

                    # ## affinity (kcal)
                    build_df[lines[_ + x][:30].strip()] = [float(-999.9999)]
                    x += 1
                elif 'None' not in lines[_ + x]:
                    # ##    affinity (kcal)
                    build_df[lines[_ + x][:30].strip()] = [
                        float(lines[_ + x][44:55])]
                    x += 1
                else:
                    x += 1
            del x

        elif ' --- Saturation States of Solid Solutions ---' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):
                if re.findall(r'\*{4}$', lines[_ + x]):
                    # ## ****** fills in the value region for numbers
                    # ## lower than -999.9999, So I am replacing them
                    # ## here with the boundry condition.

                    # ## affinity (kcal)
                    build_df[lines[_ + x][:30].strip()] = [float(-999.9999)]
                    x += 1
                elif 'None' not in lines[_ + x]:
                    # ## affinity (kcal)
                    build_df[lines[_ + x][:30].strip()] = [
                        float(lines[_ + x][44:55])]
                    x += 1
                else:
                    x += 1
            del x

        elif '    --- Fugacities ---' in lines[_]:
            x = 4
            while not re.findall('^\n', lines[_ + x]):

                if 'None' not in lines[_ + x]:  # log f
                    if re.findall(r'\*{4}', lines[_ + x]):
                        # ## ****** fills in the value region for numbers
                        # ## lower than -999.9999 and for gasses that have
                        # ## been user suppressed. So I am replacing them
                        # ## here with the boundry condition.

                        # ## affinity (kcal)
                        build_df[lines[_ + x][:30].strip()] = [float(-999.9999)]
                        x += 1
                    else:
                        build_df[grab_str(lines[_ + x], 0)] = [
                            float(lines[_ + x][28:41])]
                        x += 1
                else:
                    x += 1
            del x
            break

    # ## combine temp_s_dict containing solid moles precip, and the
    # ## build_df, which continas the affinity data.
    for _ in temp_s_dict.keys():
        # ## write temp_s_dict[_] to build_df[_]. As temp_s_dict only
        # ## contains solids that actually precipitated, the
        # ## affinity vlaues in build_df[_] can simply be overwritten
        build_df[_] = temp_s_dict[_]

    # ## set remaining es table variables
    build_df['uuid'] = [dat[0]]
    build_df['camp'] = [dat[1]]
    build_df['ord'] = [dat[2]]
    build_df['file'] = [dat[3]]
    build_df['run'] = [date]
    build_df['mineral'] = [dat[5]]

    # ## reorganize columns to match es table
    build_df = build_df[col_names]

    return run_code, build_df

def six_o_data_to_sql(conn, table, df):
    df.to_sql(table, conn, if_exists='replace', index=False)

def six_o_data_to_sql_old(conn, table, df):
    """
    Write dataframe 'df' to postgresql 'table' on connection'conn.'
    """
    # TODO: Move to DB comms, and figure out how write to table effectively
    # with pandas
    tuples = [tuple(x) for x in df.to_numpy()]
    # ##    must wrap col names in "" for pastgres to accept special
    # ## characters within names
    cols = ','.join(['"{}"'.format(_) for _ in df.columns])
    query = "INSERT INTO %s(%s) VALUES %%s" % (table, cols)
    cursor = conn.cursor()

    print('1')

    try:
        extras.execute_values(cursor, query, tuples)  # TODO: This shit won't work
        conn.commit()
        cursor.close()
    except Exception as error:
        print("Postgres Error: %s" % error)
        print("  I. fuckked. up.")
        print("  I misdialed!\n")
        conn.rollback()
        cursor.close()


# if __name__ == "__main__":
#     camp = campaign.Campaign.from_json(CAMPAIGN_FILE)
#     camp.create_env()
#     main(camp)
